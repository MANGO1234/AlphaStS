{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install lz4\n\nimport numpy as np\nimport lz4.frame\nimport struct\nimport json\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport os\nimport os.path\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nSAVES_DIR = '/kaggle/input/alphaStS-dataset'\n\nwith open(f'{SAVES_DIR}/training.json', 'r') as f:\n    training_info_raw = json.load(f)\n\ninput_len = training_info_raw['input_len']\nnum_of_actions = training_info_raw['num_of_actions']\nv_other_lens = training_info_raw['v_other_lens']\nv_other_len = sum(v_other_lens)\ntraining_info = {}\ntraining_info['iteration'] = training_info_raw['iteration']\nSLOW_WINDOW_END = training_info_raw['SLOW_WINDOW_END']\nTRAINING_WINDOW_SIZE = training_info_raw['TRAINING_WINDOW_SIZE']\nprint(training_info['iteration'])\n\nprint(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))\n\n\ndef softmax_cross_entropy_with_logits(y_true, y_pred):\n    p = y_pred\n    pi = y_true\n    zero = tf.zeros(shape=tf.shape(pi), dtype=tf.float32)\n    where = tf.less(pi, zero)\n    negatives = tf.fill(tf.shape(pi), -1000.0)\n    p = tf.where(where, negatives, p)\n    pi = tf.where(where, zero, pi)\n    loss = tf.nn.softmax_cross_entropy_with_logits(labels=pi, logits=p)\n    return loss\n\n\ndef softmax_cross_entropy_with_logits_simple(y_true, y_pred):\n    return tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n\n\ndef mse_ignoring_out_of_bound(y_true, y_pred):\n    # allow agent to output negative values to ignore training data for losses when training\n    p = y_pred\n    pi = y_true\n    zero = tf.zeros(shape=tf.shape(pi), dtype=tf.float32)\n    where = tf.less(pi, zero - 2)\n    pi = tf.where(where, p, pi)\n    return tf.keras.losses.MeanSquaredError()(p, pi)\n\n\nif os.path.exists(f'{SAVES_DIR}/iteration{training_info[\"iteration\"] - 1}'):\n    custom_objects = {\"softmax_cross_entropy_with_logits\": softmax_cross_entropy_with_logits,\n                      \"softmax_cross_entropy_with_logits_simple\": softmax_cross_entropy_with_logits_simple, \"mse_ignoring_out_of_bound\": mse_ignoring_out_of_bound}\n    with keras.utils.custom_object_scope(custom_objects):\n        model = tf.keras.models.load_model(f'{SAVES_DIR}/iteration{training_info[\"iteration\"] - 1}')\n\n\ndef get_training_samples(training_pool, iteration, file_path):\n    if not os.path.exists(file_path) and not os.path.exists(file_path + '.lz4'):\n        return\n    if os.path.exists(file_path):\n        with open(file_path, 'rb') as f:\n            content = f.read()\n    else:\n        with lz4.frame.open(file_path + '.lz4', mode='r') as f:\n            content = f.read()\n    offset = 0\n    while offset != len(content):\n        x_fmt = '>' + ('f' * input_len)\n        x = struct.unpack(x_fmt, content[offset: offset + 4 * input_len])\n        v_fmt = '>' + 'f' * (2 + v_other_len)\n        v = struct.unpack(v_fmt, content[offset + 4 * input_len:offset + 4 * (input_len + 2 + v_other_len)])\n        p_fmt = '>' + ('f' * num_of_actions)\n        p = struct.unpack(p_fmt, content[offset + 4 * (input_len + 2 + v_other_len):offset + 4 * (input_len + 2 + v_other_len + num_of_actions)])\n        offset += 4 * (input_len + 2 + v_other_len + num_of_actions)\n        target = [list(x), v[0], v[1], list(p), [v_other for v_other in v[2:]]]\n        training_pool.append((iteration, target))\n    if len(content) != offset:\n        print(f'{len(content) - offset} bytes remaining for decoding')\n        raise \"agent error\"\n\n\ntraining_pool = []\nstart_window = 0\nif training_info['iteration'] >= SLOW_WINDOW_END:\n    start_window = max(SLOW_WINDOW_END, training_info['iteration'] - TRAINING_WINDOW_SIZE)\n    for i in range(start_window, training_info['iteration'] - 1):\n        print(f'loading data from {SAVES_DIR}/iteration{i}/training_data.bin')\n        get_training_samples(training_pool, i, f'{SAVES_DIR}/iteration{i}/training_data.bin')\nget_training_samples(training_pool, training_info[\"iteration\"] - 1, f'{SAVES_DIR}/iteration{training_info[\"iteration\"] - 1}/training_data.bin')\n\nprint(f'number of samples={len(training_pool)}')\n\ntrain_iter = 10 if training_info['iteration'] < SLOW_WINDOW_END + TRAINING_WINDOW_SIZE - 1 else 5\nfor _i in range(train_iter):\n    minibatch = training_pool\n    x_train = []\n    exp_health_head_train = []\n    exp_win_head_train = []\n    policy_head_train = []\n    exp_other_heads_train = []\n    for i in range(len(v_other_lens)):\n        exp_other_heads_train.append([])\n    for _, (x, v_win, v_health, p, v_others) in minibatch:\n        x_train.append(np.asarray(x))\n        exp_health_head_train.append(np.asarray(v_health).reshape(1))\n        exp_win_head_train.append(np.asarray(v_win).reshape(1))\n        policy_head_train.append(np.asarray(p).reshape(num_of_actions))\n        for i in range(len(v_other_lens)):\n            exp_other_heads_train[i].append(np.asarray(v_others[i]).reshape(v_other_lens[i]))\n    x_train = np.asarray(x_train)\n    exp_health_head_train = np.asarray(exp_health_head_train)\n    exp_win_head_train = np.asarray(exp_win_head_train)\n    policy_head_train = np.asarray(policy_head_train)\n    for i in range(len(v_other_lens)):\n        exp_other_heads_train[i] = np.asarray(exp_other_heads_train[i])\n    target = [exp_health_head_train, exp_win_head_train, policy_head_train] + exp_other_heads_train\n    fit_result = model.fit(np.asarray(x_train), target, epochs=1)\nmodel.save(f'/kaggle/working/iteration{training_info[\"iteration\"]}')\n","metadata":{},"execution_count":null,"outputs":[]}]}